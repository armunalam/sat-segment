{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from UnimatchV2_LULC.model.semseg.dpt import DPT\n",
    "from utils.utils import unpatchify, decode_segmap, LABELS, COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patches(image: np.ndarray, patch_size: int) -> tuple[np.ndarray, tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Splits a PIL image into patches of size (patch_size, patch_size),\n",
    "    zero-padding the remaining parts if needed.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Input image.\n",
    "        patch_size (int): Size of each patch (square).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of patches with shape \n",
    "        (num_patches_vertical, num_patches_horizontal, 1, patch_size, patch_size, 3)\n",
    "    \"\"\"\n",
    "    # image = image.convert('RGB')\n",
    "    image_size = image.shape[1], image.shape[0]\n",
    "    # image_np = np.array(image)\n",
    "    image_np = image\n",
    "    h, w, c = image_np.shape\n",
    "\n",
    "    num_patches_vertical = (h + patch_size - 1) // patch_size\n",
    "    num_patches_horizontal = (w + patch_size - 1) // patch_size\n",
    "\n",
    "    patches = torch.zeros(\n",
    "        (num_patches_vertical, num_patches_horizontal, 1, patch_size, patch_size, c)\n",
    "    )\n",
    "\n",
    "    for i in range(num_patches_vertical):\n",
    "        for j in range(num_patches_horizontal):\n",
    "            y_start = i * patch_size\n",
    "            x_start = j * patch_size\n",
    "            patch = image_np[y_start:y_start +\n",
    "                             patch_size, x_start:x_start + patch_size]\n",
    "\n",
    "            # Handle padding if needed\n",
    "            padded_patch = torch.zeros(\n",
    "                (patch_size, patch_size, c))\n",
    "            padded_patch[:patch.shape[0], :patch.shape[1], :] = patch\n",
    "\n",
    "            patches[i, j, 0] = padded_patch\n",
    "\n",
    "    return patches, image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = DPT(\n",
    "    **{'encoder_size': 'base', 'features': 128, 'out_channels': [96, 192, 384, 768],\n",
    "       'nclass': 6})\n",
    "model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "model.to(device)\n",
    "\n",
    "unimatch_path = '/opt/models/exp/unimatchv2_0.pth'\n",
    "checkpoint = torch.load(\n",
    "    unimatch_path, map_location='cpu', weights_only=False)\n",
    "new_state_dict = {}\n",
    "for k, v in checkpoint['model'].items():\n",
    "    new_key = k.replace('module.', '')\n",
    "    new_state_dict[new_key] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "def predict(image: np.ndarray, patch_size: int = 518) -> tuple[Image.Image, Image.Image, list]:\n",
    "    image = image[:, :, :3]\n",
    "    original_image = image.copy()\n",
    "    \n",
    "    image = torchvision.transforms.functional.to_tensor(\n",
    "    image)\n",
    "    image = torchvision.transforms.functional.normalize(\n",
    "    image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).permute((1, 2, 0))\n",
    "\n",
    "    patch_images, image_size = make_patches(image, patch_size)\n",
    "\n",
    "    size_y, size_x, _, p_s_1, p_s_2, channels = patch_images.shape\n",
    "    patch_images = patch_images.reshape(\n",
    "        size_x * size_y, p_s_1, p_s_2, channels).permute((0, 3, 1, 2)).cuda()\n",
    "\n",
    "    output_images = []\n",
    "    count_array = np.zeros(6, dtype=np.int_)\n",
    "\n",
    "    for image in patch_images:\n",
    "        model.eval()\n",
    "\n",
    "        # image = torchvision.transforms.functional.to_tensor(\n",
    "        #     image).to(device)\n",
    "        # image = torchvision.transforms.functional.normalize(\n",
    "        #     image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).reshape(1, channels, patch_size, patch_size)\n",
    "        image = image.reshape(1, channels, patch_size, patch_size)\n",
    "\n",
    "        image = image.to(dtype=torch.float32)\n",
    "\n",
    "        output = model(image)\n",
    "        output = output.detach().max(dim=1)[1].cpu().numpy().squeeze(axis=0)\n",
    "\n",
    "        unique, counter = np.unique(output, return_counts=True)\n",
    "        count_temp = np.zeros(6, dtype=np.int_)\n",
    "        count_temp[unique] = counter\n",
    "        count_array += count_temp\n",
    "\n",
    "        output = decode_segmap(output)\n",
    "        output = Image.fromarray(output)\n",
    "        output_images.append(output)\n",
    "\n",
    "    output_images = np.stack(output_images, axis=0).reshape(\n",
    "        size_y, size_x, 1, p_s_1, p_s_2, channels)\n",
    "\n",
    "    output_image = unpatchify(output_images, image_size)\n",
    "    output_image = Image.fromarray(output_image)\n",
    "\n",
    "    labels = LABELS.get('lulc')\n",
    "    colors = [str(color) for color in COLORS.get('lulc')[1:]]\n",
    "    area = [f'{val * 4.92e-6:,.2f}' for val in count_array[1:]]\n",
    "    max_pixel = np.sum(count_array[1:]) or 1\n",
    "    count_array = [f'{val / max_pixel * 100:.2f}%' for val in count_array[1:]]\n",
    "    table = list(zip(labels, list(count_array), area, colors))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return Image.fromarray(original_image), output_image, table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sat-segment/lib/python3.12/site-packages/PIL/Image.py:3315\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3315\u001b[0m     mode, rawmode \u001b[38;5;241m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   3316\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 3), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m img \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(\n\u001b[1;32m      5\u001b[0m     img, mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# patched_images = make_patches(img, 512)[0]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m pred \u001b[38;5;241m=\u001b[39m predict(np\u001b[38;5;241m.\u001b[39masarray(img))[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# # pred\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# patched_images\u001b[39;00m\n\u001b[1;32m     11\u001b[0m pred\n",
      "Cell \u001b[0;32mIn[15], line 75\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(image, patch_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(labels, \u001b[38;5;28mlist\u001b[39m(count_array), area, colors))\n\u001b[1;32m     73\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mfromarray(original_image), output_image, table\n",
      "File \u001b[0;32m~/miniconda3/envs/sat-segment/lib/python3.12/site-packages/PIL/Image.py:3319\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3317\u001b[0m         typekey_shape, typestr \u001b[38;5;241m=\u001b[39m typekey\n\u001b[1;32m   3318\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot handle this data type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypekey_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypestr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3319\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   3320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3321\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f4"
     ]
    }
   ],
   "source": [
    "img = Image.open('/home/skeptic/web-lulc/trash/full_image.png')\n",
    "img = torchvision.transforms.functional.to_tensor(\n",
    "    img)\n",
    "img = torchvision.transforms.functional.normalize(\n",
    "    img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).permute((1, 2, 0))\n",
    "\n",
    "# patched_images = make_patches(img, 512)[0]\n",
    "pred = predict(np.asarray(img))[1]\n",
    "# # pred\n",
    "# patched_images\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sat-segment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
